{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 – Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains the sample code from chapter 4 and has been modified for CSC4505._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also requires Scikit-Learn ≥ 1.0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import sklearn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in previous chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create the `images/training_linear_models` folder (if it doesn't already exist), and define the `save_fig()` function which can be used through this notebook to save the figures in high-res:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"training_linear_models\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    We previously saw linear regression applied to GDP per capita vs life satisfaction ratings:<br>\n",
    "    $$life\\_satisfaction = \\theta_0 + \\theta_1 \\times GDP\\_per\\_capita.$$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "There were two model parameters, $\\theta_0$ and $\\theta_1$.  If we introduced additional information to the data as another feature, our model would become:\n",
    "    $$life\\_satisfaction = \\theta_0 + \\theta_1 \\times GDP\\_per\\_capita + \\theta_2 \\times new\\_feature,$$\n",
    "which has 3 model parameters.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "This can generalize to any number of features as:\n",
    "    $$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n = h_{\\theta}(x) = \\theta x$$\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "<font size=\"3\">\n",
    "We used RMSE (root mean squared error) in Chapter 2, here we will use MSE.  Let $x^{(i)} \\in X$ be training examples and $y^{(i)} \\in Y$ be their correct labels.\n",
    "    $$error = \\theta x^{(i)} - y^{(i)}$$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "    $$squared\\_error = (\\theta x^{(i)} - y^{(i)})^2$$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "    $$mean\\_squared\\_error = \\frac{1}{n}\\sum\\limits_{i=1}^n(\\theta x^{(i)} - y^{(i)})^2$$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "We use MSE to measure how \"bad\" our predictions are overall.  In this context, we call it our cost function.  Our goal is then to find values for $\\theta$ that minimize the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Normal Equation\n",
    "<br>\n",
    "\n",
    "<font size=\"3\">\n",
    "Fortunately, there exists an equation (closed-form solution) that tells us the answer!\n",
    "\n",
    "$$\\hat{\\theta} = (X^TX)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # to make this code example reproducible\n",
    "m = 100  # number of instances\n",
    "X = 2 * np.random.rand(m, 1)  # column vector\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)  # column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X_b = add_dummy_feature(X)  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = add_dummy_feature(X_new)  # add x0 = 1 to each instance\n",
    "y_predict = X_new_b @ theta_best\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "\n",
    "# extra code – beautifies Figure 4–2\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It worked!\n",
    "\n",
    "We can also use the builtin LinearRegression model to accomplish the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"cost\" of using this function as our prediction is given by our cost function.  Since we are using MSE, we look at the error for each point and square it.  We can visualize that error below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "slope = (y_predict[1][0] - y_predict[0][0])/2\n",
    "y_prediction_values = slope*X + y_predict[0][0]\n",
    "for i in range(len(X)):\n",
    "    plt.plot([X[i], X[i]], [y[i], y_prediction_values[i]], color='green', linestyle='dotted')\n",
    "\n",
    "# extra code – beautifies Figure 4–2\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: We are trying to minimize the cost:\n",
    "<br>\n",
    "    $$mean\\_squared\\_error = \\frac{1}{n}\\sum\\limits_{i=1}^n(\\theta x^{(i)} - y^{(i)})^2$$\n",
    "<br>\n",
    "and so far have learned that we can do so by applying the normal equation:\n",
    "<br>\n",
    "$$\\hat{\\theta} = (X^TX)^{-1} X^T y$$\n",
    "\n",
    "However, as the number of parameters grows, this quickly becomes inefficient.  If you double the number of parameters, the time to invert the matrix grows by ~5-8x.  By contrast, if you double the input data then you also double the number of computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Alternatively, we can begin randomly and iteratively get closer to a solution.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*GaO7X6j3coh3oNwf.png\" width=\"717\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we decide how far to step? \n",
    "- Hyperparameter choice\n",
    "\n",
    "#### Will this always work? \n",
    "- depends on learning rate and on cost function\n",
    "\n",
    "#### How do we decide which way to step? \n",
    "- gradient (derivative of multiple variables) of cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Learning Rates\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1078/1*mUGEIJRU2uxYt9rQWykQLA.png\" width=\"400\"> <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*o_0lkepFqsPg0K5VJ7m8vw.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will this always work?\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:992/1*eetskzKtJEDvzYnINgXpmQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction to Step: Gradient\n",
    "\n",
    "Partial derivative of the cost function with respect to (w.r.t.) a given parameter $\\theta_j$ tells us how much the cost function will change if we change the value of $\\theta_j$ a little bit.\n",
    "\n",
    "Every parameter will have its own partial derivative and effect on the cost.  Thus the number of dimensions of our function is the number of trainable parameters.\n",
    "\n",
    "Using MSE again, we have:\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta) = \\frac{2}{m}\\sum\\limits_{i=1}^m(\\theta x^{(i)}-y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "We want to do this for EVERY parameter, so we use the gradient:\n",
    "$$\\nabla_\\theta MSE(\\theta) = \\begin{pmatrix} \\frac{\\partial}{\\partial\\theta_0}MSE(\\theta) \\\\ \\frac{\\partial}{\\partial\\theta_1}MSE(\\theta) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial\\theta_n}MSE(\\theta) \\\\ \\end{pmatrix} = \\frac{2}{m}X^\\top (X\\theta-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the gradient, subtract from $\\theta$ as your update function:\n",
    "$$\\theta^{(\\text{next step})} = \\theta - \\eta \\nabla_\\theta MSE(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_epochs = 1000\n",
    "m = len(X_b)  # number of instances\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # randomly initialized model parameters\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – generates Figure 4–8\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "def plot_gradient_descent(theta, eta):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_epochs = 1000\n",
    "    n_shown = 20\n",
    "    theta_path = []\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch < n_shown:\n",
    "            y_predict = X_new_b @ theta\n",
    "            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n",
    "            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n",
    "        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.grid()\n",
    "    plt.title(fr\"$\\eta = {eta}$\")\n",
    "    return theta_path\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(131)\n",
    "plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.subplot(132)\n",
    "theta_path_bgd = plot_gradient_descent(theta, eta=0.1)\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plt.subplot(133)\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plot_gradient_descent(theta, eta=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Here, prediction line is shown darker each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Rate\n",
    "\n",
    "Run for \\_\\_\\_ iterations or until convergence.  If consecutive gradient steps are below a set tolerance, we say it converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What method have we learned to test multiple model options to find the best choice?\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Batch gradient descent uses ALL training data EVERY step.  Very slow with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Other extreme: update after each training instance\n",
    "\n",
    "1. Pick training instance at random\n",
    "2. Compute gradient w.r.t. training instance\n",
    "3. Make small update to $\\theta$\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_sgd = []  # extra code – we need to store the path of theta in the\n",
    "                     #              parameter space to plot the next figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "n_shown = 20  # extra code – just needed to generate the figure below\n",
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(m):\n",
    "\n",
    "        # extra code – these 4 lines are used to generate the figure\n",
    "        if epoch == 0 and iteration < n_shown:\n",
    "            y_predict = X_new_b @ theta\n",
    "            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n",
    "            plt.plot(X_new, y_predict, color=color)\n",
    "\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index : random_index + 1]\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m\n",
    "        eta = learning_schedule(epoch * m + iteration)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)  # extra code – to generate the figure\n",
    "\n",
    "# extra code – this section beautifies Figure 4–10\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n",
    "                       n_iter_no_change=100, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent\n",
    "\n",
    "In between SGD and Batch GD. Divide training data into minibatches, then repeatedly compute gradient and update for each minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates Figure 4–11\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "n_epochs = 50\n",
    "minibatch_size = 20\n",
    "n_batches_per_epoch = ceil(m / minibatch_size)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta_path_mgd = []\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for iteration in range(0, n_batches_per_epoch):\n",
    "        idx = iteration * minibatch_size\n",
    "        xi = X_b_shuffled[idx : idx + minibatch_size]\n",
    "        yi = y_shuffled[idx : idx + minibatch_size]\n",
    "        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n",
    "        eta = learning_schedule(iteration)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n",
    "         label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n",
    "         label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n",
    "         label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(r\"$\\theta_0$\")\n",
    "plt.ylabel(r\"$\\theta_1$   \", rotation=0)\n",
    "plt.axis([2.6, 4.6, 2.3, 3.4])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Algorithms\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*tSwCfYYEFqWNJirZXlblyQ.png\">\n",
    "\n",
    "NOTE: m is training instances, n is parameters here\n",
    "\n",
    "###  Which linear regression training algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "### Which linear regression training algorithm can you use if you have a training set with millions of training instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which gradient descent algorithm will reach an optimum fastest?  Which will converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about nonlinear data?  Add the square of each feature as new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for style, width, degree in ((\"r-+\", 2, 1), (\"b--\", 2, 2), (\"g-\", 1, 300)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    label = f\"{degree} degree{'s' if degree > 1 else ''}\"\n",
    "    plt.plot(X_new, y_newbig, style, label=label, linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression on features $a, b, c$ can add not only $a^2, b^2, c^2$, but also $ab, ac, bc$.  This allows the model to learn relations between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check training and validation costs as you update parameters and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.axis([0, 80, 0, 2.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model shown above underfitting or overfitting the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=10, include_bias=False),\n",
    "    LinearRegression())\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – generates Figure 4–16\n",
    "\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.axis([0, 80, 0, 2.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model shown above underfitting or overfitting the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regularized Linear Models\n",
    "\n",
    "Sometimes (such as when overfitting training data), it can be beneficial to minimize model parameters.  This can be done by adding a regularizer to the cost function such as:\n",
    "$$J(\\theta) = MSE(\\theta) + \\frac{\\alpha}{m}\\sum\\limits_{i=1}^n \\theta_i^2 = MSE(\\theta) + \\frac{\\alpha}{m} (||w||_2)^2$$\n",
    "\n",
    "### How do you think feature scaling affects regularization?  \n",
    "### Think about the housing dataset.  If we left the data unscaled and had one feature with a range of (95000, 500000) and another feature with a range of (-10,10), what happens to the respective parameter magnitudes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the quadratic dataset we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# extra code – creates the same quadratic dataset as earlier and splits it\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n",
    "X_train, y_train = X[: m // 2], y[: m // 2, 0]\n",
    "X_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n",
    "\n",
    "preprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n",
    "                              StandardScaler())\n",
    "X_train_prep = preprocessing.fit_transform(X_train)\n",
    "X_valid_prep = preprocessing.transform(X_valid)\n",
    "sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\n",
    "n_epochs = 500\n",
    "best_valid_rmse = float('inf')\n",
    "train_errors, val_errors = [], []  # extra code – it's for the figure below\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.partial_fit(X_train_prep, y_train)\n",
    "    y_valid_predict = sgd_reg.predict(X_valid_prep)\n",
    "    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n",
    "    if val_error < best_valid_rmse:\n",
    "        best_valid_rmse = val_error\n",
    "        best_model = deepcopy(sgd_reg)\n",
    "\n",
    "    # extra code – we evaluate the train error and save it for the figure\n",
    "    y_train_predict = sgd_reg.predict(X_train_prep)\n",
    "    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n",
    "    val_errors.append(val_error)\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "# extra code – this section generates Figure 4–20\n",
    "best_epoch = np.argmin(val_errors)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_valid_rmse),\n",
    "             xytext=(best_epoch, best_valid_rmse + 0.5),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(best_epoch, best_valid_rmse, \"bo\")\n",
    "plt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.axis([0, n_epochs, 0, 3.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping means stopping training when validation cost begins increasing (or stops decreasing) for \\_\\_\\_ consecutive iterations.\n",
    "\n",
    "### When using minibatch gradient descent, should you stop training immediately when validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – generates Figure 4–21\n",
    "\n",
    "lim = 6\n",
    "t = np.linspace(-lim, lim, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot([-lim, lim], [0, 0], \"k-\")\n",
    "plt.plot([-lim, lim], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-lim, lim], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.axis([-lim, lim, -0.1, 1.1])\n",
    "plt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression applies this logistic function to the output of a linear model:\n",
    "$$\\sigma(\\theta x)$$\n",
    "\n",
    "This sigmoid (S-shaped) function always outputs between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cost Function\n",
    "\n",
    "The cost function for a training instance for logistic regression is given by:\n",
    "$$c(\\theta) = \\begin{cases} -log(\\sigma(\\theta x))\\text{ if }y=1\\\\-log(1-\\sigma(\\theta x))\\text{ if }y=0\\end{cases}$$\n",
    "\n",
    "This is then computed and averaged across the whole training dataset.  This cost function is called log loss.\n",
    "\n",
    "This is a good choice of loss function because it is guaranteed to only have 1 minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)  # extra code – it's a bit too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.head(3)  # note that the instances are not shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data[[\"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n",
    "         label=\"Not Iris virginica proba\")\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\n",
    "plt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n",
    "         label=\"Decision boundary\")\n",
    "\n",
    "# extra code – this section beautifies Figure 4–23\n",
    "plt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n",
    "          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\n",
    "plt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n",
    "          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\n",
    "plt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\n",
    "plt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\n",
    "plt.xlabel(\"Petal width (cm)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates Figure 4–24\n",
    "\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(C=2, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# for the contour plot\n",
    "x0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "\n",
    "# for the decision boundary\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n",
    "             / log_reg.coef_[0, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\n",
    "plt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the decision boundary away from 50% if we want to predict more or less aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train multiple logistic regression models together to form one multiclass classifier by combining them using a softmax function.\n",
    "\n",
    "Score for class k, $s_k$ given by:\n",
    "$$s_k(x) = \\theta^{(k)}x$$\n",
    "\n",
    "Then overall prediction given by comparing scores across classes:\n",
    "$$\\hat{p}_k = \\sigma(s(x))_k = \\frac{\\exp(s_k(x))}{\\sum\\limits_{j=1}^K \\exp(s_j(x))}$$\n",
    "\n",
    "If we are only concerned with which class is predicted, it is whichever has the highest score.\n",
    "\n",
    "### Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two logistic regression classifiers or one softmax regression classifier?\n",
    "\n",
    "### Why do we use softmax instead of argmax when training?  What is the gradient of an argmax function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "softmax_reg = LogisticRegression(C=30, random_state=42)\n",
    "softmax_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax_reg.predict_proba([[5, 2]]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates Figure 4–25\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n",
    "\n",
    "x0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "                     np.linspace(0, 3.5, 200).reshape(-1, 1))\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=\"hot\")\n",
    "plt.clabel(contour, inline=1)\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.axis([0.5, 7, 0, 3.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
